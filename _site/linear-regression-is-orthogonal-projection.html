<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="/assets/images/geometry.jpg"/><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Ordinary Least Squares is Orthogonal Projection | Deylemma</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Ordinary Least Squares is Orthogonal Projection" />
<meta name="author" content="Daniel Deychakiwsky" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post visualizes the equivalence of two perspectives on estimating the unknown parameters in a simple linear regression model, ordinary least squares (OLS) and orthogonal projection (OP)." />
<meta property="og:description" content="This post visualizes the equivalence of two perspectives on estimating the unknown parameters in a simple linear regression model, ordinary least squares (OLS) and orthogonal projection (OP)." />
<link rel="canonical" href="http://localhost:4000/linear-regression-is-orthogonal-projection" />
<meta property="og:url" content="http://localhost:4000/linear-regression-is-orthogonal-projection" />
<meta property="og:site_name" content="Deylemma" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-08T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"http://localhost:4000/linear-regression-is-orthogonal-projection","headline":"Ordinary Least Squares is Orthogonal Projection","dateModified":"2020-12-08T00:00:00-05:00","datePublished":"2020-12-08T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/linear-regression-is-orthogonal-projection"},"author":{"@type":"Person","name":"Daniel Deychakiwsky"},"description":"This post visualizes the equivalence of two perspectives on estimating the unknown parameters in a simple linear regression model, ordinary least squares (OLS) and orthogonal projection (OP).","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Deylemma" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Deylemma</a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Ordinary Least Squares is Orthogonal Projection</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-12-08T00:00:00-05:00" itemprop="datePublished">
        Dec 8, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Daniel Deychakiwsky</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This post visualizes the equivalence of two perspectives
on estimating the unknown parameters in a simple linear
regression model, ordinary least squares (OLS) and
orthogonal projection (OP).</p>

<ul id="markdown-toc">
  <li><a href="#context" id="markdown-toc-context">Context</a></li>
  <li><a href="#ols--op" id="markdown-toc-ols--op">OLS &amp; OP</a>    <ul>
      <li><a href="#the-big-picture" id="markdown-toc-the-big-picture">The Big Picture</a>        <ul>
          <li><a href="#fig-1" id="markdown-toc-fig-1">Fig. 1</a></li>
        </ul>
      </li>
      <li><a href="#toy-dataset" id="markdown-toc-toy-dataset">Toy Dataset</a>        <ul>
          <li><a href="#fig-2" id="markdown-toc-fig-2">Fig. 2</a></li>
        </ul>
      </li>
      <li><a href="#ols-perspective" id="markdown-toc-ols-perspective">OLS Perspective</a>        <ul>
          <li><a href="#fig-3" id="markdown-toc-fig-3">Fig. 3</a></li>
          <li><a href="#fig-4" id="markdown-toc-fig-4">Fig. 4</a></li>
        </ul>
      </li>
      <li><a href="#op-perspective" id="markdown-toc-op-perspective">OP Perspective</a>        <ul>
          <li><a href="#fig-5" id="markdown-toc-fig-5">Fig. 5</a></li>
          <li><a href="#fig-6" id="markdown-toc-fig-6">Fig. 6</a></li>
          <li><a href="#fig-7" id="markdown-toc-fig-7">Fig. 7</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#desmos" id="markdown-toc-desmos">Desmos</a></li>
</ul>

<h2 id="context">Context</h2>

<p>In order to get more out of this post, you may want to brush up on:</p>

<ol>
  <li>Wikipedias on <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares</a>, <a href="https://en.wikipedia.org/wiki/Vector_projection">vector projection</a>,
$L^2$ <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norm</a>, and <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a>.</li>
  <li><a href="https://medium.com/@vladimirmikulik/why-linear-regression-is-a-projection-407d89fd9e3a">Vladimir Mikulik’s post</a> on “Why Linear Regression
is a projection”.</li>
  <li><a href="https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b">Andrew Chamberlain’s post</a> on
“The Linear Algebra View of Least-Squares Regression”.</li>
</ol>

<h2 id="ols--op">OLS &amp; OP</h2>

<h3 id="the-big-picture">The Big Picture</h3>

<p><a href="#fig-1">Fig 1.</a> is a compact and interactive
<a href="https://www.desmos.com/calculator/eowrcpdore">visualization</a> that superimposes the two perspectives modeling a
<a href="#toy-dataset">toy dataset</a>. The remainder of this post examines
each perspective in greater detail.</p>

<h4 id="fig-1">Fig. 1</h4>

<iframe src="https://www.desmos.com/calculator/eowrcpdore?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder="0">
</iframe>

<h3 id="toy-dataset">Toy Dataset</h3>

<p>For ease of intuition, our toy dataset is kept simple and
low-dimensional; just two data points $\in \mathbb{R}^2$.</p>

<p>As column vectors:</p>

\[\vec{X_1} = \begin{bmatrix}2 \\ 1\end{bmatrix}
\quad \textrm{and} \quad
\vec{X_2} = \begin{bmatrix}3 \\ 3\end{bmatrix}
\tag{1} \label{1}\]

<p>As a matrix:</p>

\[\mathbf{X} = \begin{bmatrix}2 &amp; 3 \\ 1 &amp; 3\end{bmatrix}
\tag{2} \label{2}\]

<h4 id="fig-2">Fig. 2</h4>

<p><img src="assets/images/linear_regression_is_orthogonal_projection/toy_data_x.png" alt="toy_data_x" /></p>

<p><a href="#fig-2">Fig 2.</a> shows how the vector representations
(\ref{1}) translate to the cartesian plane as the
first dimension represents the canonical $x$-axis
while the second dimension represents the
canonical $y$-axis.</p>

<h3 id="ols-perspective">OLS Perspective</h3>

<p>In the context of machine learning,
it is common to view OLS linear regression
as a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> problem.
Here’s a hand-wavy :) overview.</p>

<p>The model is presented inputs for which
it makes corresponding predictions (outputs).
An error signal is calculated by aggregating the squared difference
of the model’s outputs against corresponding “true” values that have been
observed or are known a priori (a.k.a. labels).
Hence the term “supervised”, the error signal is used to correct
(as a teacher often corrects a student) the model parameters.
This training routine loops until the
model parameters converge. By minimizing the error signal,
the model learns optimal parameters.</p>

\[x=2.0
\\
y=1.0
\\
\hat{y} = f(x;\beta) = 1.2
\\
err(y, \hat{y}) = (1.0 - 1.2)^2
\tag{3} \label{3}\]

<p>\ref{3} shows a hypothetical example of a single instance
error calculation, $err$, for some model input, $x$,
some model prediction (output),
$\hat{y}$; a linear function of $x$ parameterized by $\beta$,
and some label, $y$.</p>

<p>On top of our already simplified dataset, we’ll also simplify model
complexity by omitting the bias/intercept term, i.e., restricting our model
to only <em>one learnable parameter</em>. Let’s train the model.</p>

<h4 id="fig-3">Fig. 3</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'weights/parameters'</span><span class="p">,</span> <span class="n">reg</span><span class="p">.</span><span class="n">coef_</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weights/parameters array([[0.84615385]])
</code></pre></div></div>

<p><a href="#fig-3">Fig 3.</a> prints the resulting parameters (just one in our case)
of an OLS <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">LinearRegression</a> implementation written in Python using
a popular machine-learning Python package (<a href="https://scikit-learn.org/stable/">scikit-learn</a>) after being
fit to (learning) our <a href="#toy-dataset">toy dataset</a>.</p>

<h4 id="fig-4">Fig. 4</h4>

<p><img src="assets/images/linear_regression_is_orthogonal_projection/ols_regression_fit.png" alt="ols_regression_fit" /></p>

<p><a href="#fig-4">Fig 4.</a> shows the OLS linear regression model fit to our
<a href="#toy-dataset">toy dataset</a>. The hyperplane (line of best fit)
has a slope of $0.84615385$ as expected per output of <a href="#fig-3">Fig 3.</a>
If we didn’t omit the bias/intercept term and we
let the model learn another degree of freedom (another parameter),
the solution would yield a hyperplane that fits the data perfectly,
i.e., the hyperplane would be free to adjust itself to interpolate
(intersect) exactly both data points. See <a href="https://en.wikipedia.org/wiki/Polynomial_interpolation">polynomial interpolation</a>
for proof of uniqueness.</p>

<p>How does all of that translate to OP? Let’s take a look.</p>

<h3 id="op-perspective">OP Perspective</h3>

<p>Here’s another hand-wavy :) overview.</p>

<p>The learning process under the lens of OP
reduces to solving a system of linear equations;
so we’ll need to frame
our <a href="#toy-dataset">toy dataset</a> as such by vectorizing the linear model, e.g.,
the model’s outputs will be a linear function of the inputs
and the learned parameters. We can represent our model inputs and
labels as vectors $\in \mathbb{R}^2$.</p>

\[\vec{Y_1} = \begin{bmatrix}2 \\ 3\end{bmatrix}
\quad \textrm{and} \quad
\vec{Y_2} = \begin{bmatrix}1 \\ 3\end{bmatrix}
\tag{4} \label{4}\]

<p>To disambiguate \ref{1} from \ref{4}, note that
$\vec{Y_1}$ consists of the first dimensions of $\vec{X_1}$
and $\vec{X_2}$ while $\vec{Y_2}$ consists of the second
dimensions of $\vec{X_1}$ and $\vec{X_2}$. In other words,
$\vec{Y_1}$ is a vector of our model inputs and $\vec{Y_2}$
is a vector of our model labels.</p>

<h4 id="fig-5">Fig. 5</h4>

<p><img src="assets/images/linear_regression_is_orthogonal_projection/toy_data_x_y.png" alt="toy_data_x_y" /></p>

<p><a href="#fig-5">Fig 5.</a> shows how \ref{1} and \ref{4},
together, translate to the cartesian plane.</p>

<p>The equation we need to solve to model our data is:</p>

\[\vec{Y_1}\vec{\beta} \approx \vec{Y_2}
\tag{5} \label{5}\]

<p>It’s best practice to validate the shapes of the
matrices/vectors when operating on them.
$\vec{Y_1}$ is $2 \times 1$, $\vec{\beta}$ is $1 \times 1$,
(recall that we’re omitting the bias/intercept term),
and so $\vec{Y_2}$ checks out to be $2 \times 1$.</p>

<p>Intuitively, \ref{5} tells us that $\vec{Y_2}$ is
<strong>approximately</strong> equal to a scaled version
of $\vec{Y_1}$ and that scaling factor is our
learned parameter in $\vec{\beta}$.
The approximation is important because
$\vec{Y_1}\vec{\beta} = \vec{Y_2}$
<strong>will only be true if</strong> $\vec{Y_2}$ can be exactly
expressed as a scaled version of $\vec{Y_1}$.
In practice this is seldom true. In our example,
it certainly isn’t (see <a href="#fig-6">Fig 6.</a>).</p>

<h4 id="fig-6">Fig. 6</h4>

<p><img src="assets/images/linear_regression_is_orthogonal_projection/scaled_vector.png" alt="scaled_vector" /></p>

<p><a href="#fig-6">Fig 6.</a> shows a subset of scaled versions
of $\vec{Y_1}$ with the dashed green line and one randomly
chosen realization with the orange mark. That line
actually extends to infinity in both directions making up
a <a href="https://en.wikipedia.org/wiki/Linear_subspace">vector subspace</a>. Notice how $\vec{Y_2}$ does not fall
within that subspace? This is why we need to <strong>approximate</strong>
a solution to the system and why we used the $\approx$
symbol in \ref{5}.</p>

<p>It turns out that the best approximation we can get
is the OP of $\vec{Y_2}$ onto $\vec{Y_1}$’s subspace.
Omitting mathematical proofs, let’s visualize that claim.
We can define the distance from $\vec{Y_2}$ to $\vec{Y_1}$’s
subspace as a function of some scalar, $\beta$, which computes
the distance from $\vec{Y_2}$ to a scaled version of $\vec{Y_1}$.</p>

\[\delta(\beta) = euclidean\_distance(\vec{Y_2}, \beta \cdot \vec{Y_1})
\tag{6} \label{6}\]

<h4 id="fig-7">Fig. 7</h4>

<iframe src="https://www.desmos.com/calculator/00manfmzno?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder="0">
</iframe>

<p><a href="#fig-7">Fig 7.</a> shows the distance function, $\delta(\beta)$,
and corresponding input-output tuple, ($\beta, \delta(\beta)$),
in red; of some difference-vector, in dotted orange.
The difference-vector is the vector between $\vec{Y_2}$ and some vector
(varied, indicated by the orange dotted line and mark)
falling within $\vec{Y_1}$’s subspace. Notice that
the distance function is minimized when the difference-vector
is orthogonal to $\vec{Y_1}$’s subspace. <strong>The
value of $\beta$, at the minimum is $0.84615385$, exactly the
same solution observed in <a href="#fig-3">Fig 3.</a> and <a href="#fig-4">Fig 4.</a></strong>
with OLS.</p>

<p>The term “regression” has a concrete definition in the field of
statistics but what does it mean to “regress” one variable “onto/against”
another? My answer is OP. In this example we “regressed” our
dependent variable, encoded as $\vec{Y_2}$, “onto/against” our 
explanatory variable, encoded as $\vec{Y_1}$.</p>

<p>In hindsight, this duality may not come as such a surprise after all.
If we consider the objective function from each perspective, they
both seek to minimize a similar, if not the same, <em>thing</em>. The only
difference is how the problem is framed. I’ll leave it up to you
as the reader to make the comparison between <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a>
and the $L^2$ <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norm</a> of the difference-vector, i.e.,
$||\vec{Y_1} - \beta \cdot \vec{Y_2}||_2$. The differences
come from applying normalizing constants and/or monotonic transformations,
both of which have no effect on minimization.</p>

<p>Stitching all of that together, we can circle back to
<a href="#fig-1">Fig 1.</a> and watch the two perspectives “move” together.
Hopefully now, it is easier to interpret and makes for an
elegant visualization.</p>

<h2 id="desmos">Desmos</h2>

<p>I built the visualizations for this post with <a href="https://www.desmos.com/">Desmos</a>,
an incredible graphing web application that enables
users to visualize, study, and learn mathematics.</p>


  </div><a class="u-url" href="/linear-regression-is-orthogonal-projection" hidden></a>
</article><script>
  (function () {
    let script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML";

    const config =  'MathJax.Hub.Config({' +
                    'tex2jax: {' +
                      'inlineMath: [ [\'$\',\'$\'] ],' +
                      'processEscapes: true' +
                    '}' +
                  '});'

    if (window.opera) {
      script.innerHTML = config
    } else {
      script.text = config
    }

    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
<!--        <p class="feed-subscribe">-->
<!--          <a href="/feed.xml">-->
<!--            <svg class="svg-icon orange">-->
<!--              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>-->
<!--            </svg><span>Subscribe</span>-->
<!--          </a>-->
<!--        </p>-->
        <ul class="contact-list">
          <li class="p-name">Daniel Deychakiwsky</li>
          <li><a class="u-email" href="mailto:d.deychak@gmail.com">d.deychak@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>An Engineering Blog
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/deychak" title="deychak"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.instagram.com/danieldeychak" title="danieldeychak"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#instagram"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/danieldeychak" title="danieldeychak"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
