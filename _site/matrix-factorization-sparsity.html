<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="/assets/images/geometry.jpg"/><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Matrix Factorization Sparsity | Deylemma</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Matrix Factorization Sparsity" />
<meta name="author" content="Daniel Deychakiwsky" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post explores how implicit user-item interaction sparsity poses a challenge for matrix factorization (model-based) collaborative filtering recommender systems." />
<meta property="og:description" content="This post explores how implicit user-item interaction sparsity poses a challenge for matrix factorization (model-based) collaborative filtering recommender systems." />
<link rel="canonical" href="http://localhost:4000/matrix-factorization-sparsity" />
<meta property="og:url" content="http://localhost:4000/matrix-factorization-sparsity" />
<meta property="og:site_name" content="Deylemma" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-02T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Matrix Factorization Sparsity" />
<script type="application/ld+json">
{"headline":"Matrix Factorization Sparsity","url":"http://localhost:4000/matrix-factorization-sparsity","dateModified":"2022-01-02T00:00:00-06:00","datePublished":"2022-01-02T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/matrix-factorization-sparsity"},"author":{"@type":"Person","name":"Daniel Deychakiwsky"},"description":"This post explores how implicit user-item interaction sparsity poses a challenge for matrix factorization (model-based) collaborative filtering recommender systems.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Deylemma" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Deylemma</a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Matrix Factorization Sparsity</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-01-02T00:00:00-06:00" itemprop="datePublished">
        Jan 2, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Daniel Deychakiwsky</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This post explores how implicit user-item interaction sparsity poses
a challenge for matrix factorization (model-based) collaborative filtering
recommender systems.</p>

<ul id="markdown-toc">
  <li><a href="#primer" id="markdown-toc-primer">Primer</a></li>
  <li><a href="#matrix-factorization" id="markdown-toc-matrix-factorization">Matrix Factorization</a></li>
  <li><a href="#sparsity" id="markdown-toc-sparsity">Sparsity</a>    <ul>
      <li><a href="#problematic" id="markdown-toc-problematic">Problematic?</a></li>
      <li><a href="#simulations" id="markdown-toc-simulations">Simulations</a></li>
    </ul>
  </li>
</ul>

<h1 id="primer">Primer</h1>

<p>This post assumes understanding of the general <em>recommendations</em>
machine learning (ML) problem, basic modeling approaches,
and commonly used evaluation metrics.
For a refresher, check out Google’s recommender systems mini-<a href="https://developers.google.com/machine-learning/recommendation/collaborative/basics">course</a>
which introduces Content-Based Filtering (CBF),
Collaborative Filtering (CF) with Matrix Factorization (MF),
and Deep Neural Networks (DNN). The remainder of this post 
focuses on MF for <strong>sparse implicit</strong> user-item interactions.</p>

<h1 id="matrix-factorization">Matrix Factorization</h1>

<p>CF with MF is a type of model-based (parametric) algorithm that
decomposes $\mathbf{V}$ (user-item interaction matrix)
into a product of two matrices, $\mathbf{W}$ (latent user embeddings)
and $\mathbf{H}$ (latent item embeddings), by minimizing
some flavor of reconstruction error. Although MF is
inherently linear, it is a “tried and true” technique 
that is often reported to produce serendipitous recommendations.</p>

\[\mathbf{W}\mathbf{H}^\top \approx \mathbf{V}
\tag{1} \label{1}\]

<p>The algorithm exposes a hyperparameter that serves to expand or
contract the dimensionality of the columns in $\mathbf{W}$ 
and the rows in $\mathbf{H}$. It can be set so that $\mathbf{W}$ and 
$\mathbf{H}$ become low-rank factors of $\mathbf{V}$, 
<strong>forcing a compressed encoding that captures latent structure</strong>
(important information) for approximating $\mathbf{V}$.</p>

<p><img src="assets/images/matrix_factorization_sparsity/mf.png" alt="mf" /></p>

<h1 id="sparsity">Sparsity</h1>

<p>At production grade scale it’s common to produce recommendations
for millions of users and items based off of sparse implicit 
interactions, e.g., purchases, streams, etc.</p>

<p>Imagine a scenario with just $24$ unique items and $24$ unique users.
If each user purchased 3 unique items, the number of 
non-zeros in this interaction matrix is $24 * 3 = 72$
while the remaining $(24 ∗ 24) - 72 = 504$ entries are all zeros.
That’s a sparsity of $1 - (72 / (24 * 24)) = 0.875$. In other words, 
$87.5\%$ of the interaction matrix entries are zeros.</p>

\[\begin{bmatrix}
0 &amp;  0  &amp; \ldots &amp; 0\\
0  &amp;  1 &amp; \ldots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
1  &amp;   0       &amp;\ldots &amp; 0
\end{bmatrix}
\tag{2} \label{2}\]

<p>MF models are fairly resilient 
but extreme cases can become problematic.
It’s common to subsample (users and / or items) and / or collect 
multi-channel interaction data (from other implicit data 
sources, e.g., time-on-page) in hopes of reducing sparsity. 
Even after applying user subsampling,
authors of the noteworthy Neural Collaborative Filtering (<a href="https://arxiv.org/pdf/1708.05031.pdf">NCF</a>) paper 
reported a sparsity of $99.73\%$ on Pinterest data.</p>

<blockquote>
  <p>The original data is very large but highly sparse. 
For example, over 20% of users have only one pin, making it difficult
to evaluate collaborative filtering algorithms. As such, we
filtered the dataset in the same way as the MovieLens data
that retained only users with at least 20 interactions (pins).</p>
</blockquote>

<h2 id="problematic">Problematic?</h2>

<p>ML models are only as good as the data that powers them
and MF is no exception to that rule. To demonstrate why sparsity
is a problem, let’s consider a thought experiment.</p>

<p>We’ll continue with the scenario from above, $24$ users and $24$ items, 
but add an evil twist so that each user has purchased only one
distinct item that no other user has purchased. We could reorder the
users (rows of the interaction matrix) to arrive at the 
identity matrix, i.e., the canonical orthonormal basis $\in \mathbb{R}^{24\times24}$
that is linearly independent by definition. In other words, there isn’t
much to learn as the unit vectors point in orthogonal directions.</p>

\[\mathbf{I}_{24} =
\begin{bmatrix}
1 &amp;  0  &amp; \ldots &amp; 0\\
0  &amp;  1 &amp; \ldots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0  &amp;   0       &amp;\ldots &amp; 1
\end{bmatrix}
\tag{3} \label{3}\]

<p>If we train the model with <code class="language-plaintext highlighter-rouge">factors=24</code> 
(the hyperparameter discussed earlier), what do you think 
the model learns to do? It ends up learning exactly what it’s
supposed to; its best reconstruction of the interaction (identity)
matrix and it does so by <strong>inverting the item factor matrix</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">implicit.als</span> <span class="k">as</span> <span class="n">mf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="n">sp</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">mf</span><span class="p">.</span><span class="n">AlternatingLeastSquares</span><span class="p">(</span><span class="n">factors</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">m</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sp</span><span class="p">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">24</span><span class="p">)))</span>
<span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">user_factors</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">item_factors</span><span class="p">.</span><span class="n">T</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># True
</span></code></pre></div></div>

\[\mathbf{W}(\mathbf{H}^\top)^{-1} \approx \mathbf{I}
\tag{4} \label{4}\]

<p>Although the inversion is obvious given $\mathbf{W} \in \mathbb{R}^{24\times24}$, 
$\mathbf{H} \in \mathbb{R}^{24\times24}$, and $\mathbf{I}_{24}$, this degenerate case 
highlights how helpless the model becomes with no correlation or interesting 
signal in the data.</p>

<h2 id="simulations">Simulations</h2>

<p>Speaking of “signals” and “correlations”, the user-item interaction matrix
is exactly that. In practice, the interactions we observe follow some natural
generative process. If we knew the process we wouldn’t need the model.</p>

<p>Let’s generate a synthetic interaction matrix by stacking <a href="https://en.wikipedia.org/wiki/Sine_wave">sine waves</a>.
Here’s what a $1$, $2$, and $3$ Hz sine wave sampled at $1000$ Hz look like.</p>

<p><img src="assets/images/matrix_factorization_sparsity/sine_waves.png" alt="sine_waves" /></p>

<p>We layer the first $24$ harmonics (integer multiple increasing frequencies)
of a $1$ Hz sine wave <em>intentionally undersampled</em> at $25$ Hz to
produce a non-random pattern induced by the waves and their aliases.</p>

<p><img src="assets/images/matrix_factorization_sparsity/interactions.png" alt="interactions" /></p>

<p>By quantizing the amplitudes to $\{0, 1\}$, 
we end up with a bitmap that we’ll use as our
interaction matrix. Note that entries in the 
interaction matrix are not limited to discrete 
values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">interactions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
    <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">f</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">interactions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">delete</span><span class="p">(</span><span class="n">interactions</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">interactions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">delete</span><span class="p">(</span><span class="n">interactions</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">interactions</span><span class="p">[</span><span class="n">interactions</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">interactions</span><span class="p">[</span><span class="n">interactions</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
</code></pre></div></div>

<p><img src="assets/images/matrix_factorization_sparsity/interactions_bitmap.png" alt="interactions_bitmap" /></p>

<p>The spirals that emerge from the sine waves’ auto
and harmonic correlations jump out because
the brain is a pattern recognition (correlating)
machine. Don’t you think you could get close to drawing it
just by looking at it for a few seconds?
The sparsity of this matrix is $51.3\%$, about half 
of the entries are non-zeros. Since it is noticeably 
correlated and not relatively 
sparse, this will be trivial for MF - but what if we
started loosing signal, literally, by randomly zeroing
out non-zero entries and thereby increasing sparsity?</p>

<p><img src="assets/images/matrix_factorization_sparsity/interactions_removed.png" alt="interactions_removed" /></p>

<p>Let’s run a monte-carlo simulation to investigate. We’ll make it
even easier by equipping the model with more than enough parameters by
sticking with <code class="language-plaintext highlighter-rouge">factors=24</code> (from above) so that the model is free to
factorize the square interaction matrix into two other square matrices 
instead of two lower-rank matrices. We’ll report performance using 
standard ranking@10 evaluation metrics against a random $80\%-20\%$ 
train-test split.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">implicit.als</span> <span class="k">as</span> <span class="n">mf</span>
<span class="kn">import</span> <span class="nn">implicit.evaluation</span> <span class="k">as</span> <span class="n">ev</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="n">sp</span>

<span class="n">n_users</span> <span class="o">=</span> <span class="mi">24</span>
<span class="n">n_items</span> <span class="o">=</span> <span class="mi">24</span>
<span class="n">n_factors</span> <span class="o">=</span> <span class="mi">24</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">24</span>
<span class="n">n_el</span> <span class="o">=</span> <span class="n">n_users</span> <span class="o">*</span> <span class="n">n_items</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">shuffle</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">sparsities</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.91</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="k">for</span> <span class="n">sparsity</span> <span class="ow">in</span> <span class="n">sparsities</span><span class="p">:</span>
    <span class="n">ranking_metrics</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">interactions</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">n_z</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_el</span> <span class="o">*</span> <span class="n">sparsity</span><span class="p">)</span>
        <span class="n">r_z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_el</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_z</span><span class="p">)</span>
        <span class="n">i</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">r_z</span><span class="p">,</span> <span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_items</span><span class="p">))]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">trn_i</span><span class="p">,</span> <span class="n">tst_i</span> <span class="o">=</span> <span class="n">ev</span><span class="p">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">sp</span><span class="p">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">mf</span><span class="p">.</span><span class="n">AlternatingLeastSquares</span><span class="p">(</span><span class="n">factors</span><span class="o">=</span><span class="n">n_factors</span><span class="p">)</span>
        <span class="n">m</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trn_i</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">ev</span><span class="p">.</span><span class="n">ranking_metrics_at_k</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">trn_i</span><span class="p">,</span> <span class="n">tst_i</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ranking_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    <span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ranking_metrics</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="assets/images/matrix_factorization_sparsity/sparsity_sim.png" alt="sparsity_sim" /></p>

<p>Initially, when sparsity is low, the model
performs incredibly well as the factorization
problem is easy (correlation and coverage).
As sparsity decreases, the model performance
degrades as the signal in the data gets
fragmented. Note that the error
fans out as sparsity increases because
randomness begins to obfuscate the model’s 
understanding of the remaining interactions.</p>

<p>Let’s try this again but make the problem 
harder by shuffling the rows of the interaction
matrix that we synthesized in the previous step.</p>

<p><img src="assets/images/matrix_factorization_sparsity/interactions_shuffled.png" alt="interactions_shuffled" /></p>

<p><img src="assets/images/matrix_factorization_sparsity/sparsity_sim_shuffled.png" alt="sparsity_sim_shuffled" /></p>

<p>The model doesn’t perform nearly as well when sparsity is low.
Similar to the previous result, the error fans out as sparsity increases,
but the bands are all-around larger. This is because the row-wise shuffle operation 
decorrelated the signal along that axis, injecting randomness apriori.</p>


  </div><a class="u-url" href="/matrix-factorization-sparsity" hidden></a>
</article><script>
  (function () {
    let script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML";

    const config =  'MathJax.Hub.Config({' +
                    'tex2jax: {' +
                      'inlineMath: [ [\'$\',\'$\'] ],' +
                      'processEscapes: true' +
                    '}' +
                  '});'

    if (window.opera) {
      script.innerHTML = config
    } else {
      script.text = config
    }

    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
<!--        <p class="feed-subscribe">-->
<!--          <a href="/feed.xml">-->
<!--            <svg class="svg-icon orange">-->
<!--              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>-->
<!--            </svg><span>Subscribe</span>-->
<!--          </a>-->
<!--        </p>-->
        <ul class="contact-list">
          <li class="p-name">Daniel Deychakiwsky</li>
          <li><a class="u-email" href="mailto:d.deychak@gmail.com">d.deychak@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>An Engineering Blog
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/deychak" title="deychak"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.instagram.com/danieldeychak" title="danieldeychak"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#instagram"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/danieldeychak" title="danieldeychak"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
